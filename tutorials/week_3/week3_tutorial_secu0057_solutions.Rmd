---
title: "Tutorial, Week 3: Text mining 1"
subtitle: "SECU0057"
author: "B Kleinberg, J Kamps"
date: "30 Jan 2020"
output: html_notebook
urlcolor: blue
---

Aims of this tutorial:

- understanding text data through numerical representations
- examining a corpus of UK rap music
- calculating and comparing readability scores

_Note: this tutorial assumes that you have the packages `quanteda` and `stringr` installed._

## Task 1: Understanding text data through numerical representations

Before we can work with text data in a more advanced manner, it is good to understand how text data is represented numerically. R - and quanteda specifically - contain numerous "built-in" datasets that you can use to understand concepts that you have learned in the lectures.

To get an overview of the packages that quanteda provides, have a look at the [online docs](https://quanteda.io/reference/index.html#section-data).

With the command below, you can look at the dataset of all inaugural speeches of US presidents (uncomment and run):

```{r}
library(quanteda)
library(stringr)
summary(data_corpus_inaugural)
```

_Note: don't forget to load quanteda with the `library(...)` command._

To access the actual texts, you can simply index the object: `data_corpus_inaugural[1]`. For now, rename the object to a variable called `US_speeches`:

```{r}
US_speeches = data_corpus_inaugural
US_speeches[1]
```

Find answers to the following questions:

1. Which text has the highest number of characters per word? And whiich one the lowest?

```{r}
# Option 1 (preferred):

## add a new column to the dataframe that contains the text data
### this line creates a new variable "char_per_word" in the "US_speeches$documents" dataframe

US_speeches$documents$char_per_word = nchar(US_speeches$documents$texts)/ntoken(US_speeches$documents$texts)

## use which.max to find the index (here: row) of the highest value
index_of_max = which.max(US_speeches$documents$char_per_word)
index_of_max

## now return the row of that index

US_speeches$documents[index_of_max,]

```

```{r}
# Option 2:
cpw <- function(text) {
  round(nchar(text) / ntoken(text, remove_punct = TRUE),2)
}

us_texts <- US_speeches$documents$texts
us_cpw = cpw(us_texts)

# Answers stored in these variables
max_cpw = which(us_cpw == max(us_cpw))
min_cpw = which(us_cpw == min(us_cpw))

max_cpw
min_cpw
```


2. In which speech was the most punctuation used?

```{r}
# create three columns: 
## 1. no. of tokens with punct.
## 2. no. of tokens without punct.
## 3. substraction of 1. and 2.

US_speeches$documents$length_with_punct = ntoken(US_speeches$documents$texts)
US_speeches$documents$length_without_punct = ntoken(US_speeches$documents$texts, remove_punct=T)
US_speeches$documents$count_punct = US_speeches$documents$length_with_punct - US_speeches$documents$length_without_punct

# same as above: find the index of the max value and show the row
US_speeches$documents[which.max(US_speeches$documents$count_punct),]
```


3. Across all speeches, which token was used the most often?

```{r}
# create a DFM and find the top features
us_dfm <- dfm(US_speeches, remove_punct = TRUE)

most_used = topfeatures(us_dfm)[1]
most_used
```


Task 2: Re-examining Zipf's law in US presidents' speeches

From the required preparation, you will remember the essence of [Zipf's Law](https://www.youtube.com/watch?v=fCn8zs912OE). 

Answer these two questions:

1. Does the US speeches corpus support the finding of the 20 most common words in English language?

```{r}
top20_us <- topfeatures(us_dfm, n = 20)
top20_us
```

Does this finding surprise you? What could explain this?

2. For the first 10 most frequently occurring words in the corpus, does Zipf's Law apply? 

Here we will simplify Zipf's Law to $freq_w \propto \frac{1}{rank_w}$, with $w$ being a given word. ($\propto$ stands for "proportional to").

```{r}
top10_us = topfeatures(us_dfm, n = 10)
zipfs_prediction = round(top10_us[1] / 1:10)

plot(top10_us, zipfs_prediction, xlab = 'Observed freq.', ylab = 'Pred. acc. to Zipf')
```

```{r}
plot(1:10, top10_us, xlab='Rank', ylab='Frequency')
```


## Task 3: Examining a corpus of UK rap music

For a research project related to a [study of UK drill music](https://arxiv.org/pdf/1911.01324.pdf), we have collated a dataset of UK rap music (non-drill). 

You can access the dataset from: [https://github.com/ben-aaron188/UCL_SECU0050/blob/master/data/uk_rap_corpus.RData](https://github.com/ben-aaron188/UCL_SECU0050/blob/master/data/uk_rap_corpus.RData). Download it to the folder where this tutorial file is located and the read the `.RData` file into your R workspace through the command below (uncomment and run):

```{r}
## Note: you need to set this path to the file location on your machine
load('/Users/bennettkleinberg/GitHub/UCL_SECU0050/data/uk_rap_corpus.RData')
```

You now have a corpus object in your workspace called `uk_rap_corpus`. _(The ID variable is structured as follows: `artistname_songname`)_.

Calculate the TFIDF for the whole corpus to answer the questions below:

1. Which token has the highest TFIDF value in the song "Borders" by "M.I.A."?
2. Choose one token of your choice and plot its TFIDF value for all songs? Did you expect this finding?
3. Visualise the relationship between DF, IDF and log(IDF) as done in the lecture. Use the 100 words with the highest document frequency.

```{r}
# create a DFM
rap_dfm = dfm(uk_rap_corpus, remove_punct = TRUE)

# get the TFIDF
rap_tfidf = dfm_tfidf(rap_dfm, scheme_tf = 'count', scheme_df = 'inverse', k=1)

# inspect the first 10 rows and first 10 columns
rap_tfidf[1:10, 1:10]
```

```{r}
# Question 1
borders_tfidf = dfm_subset(rap_dfm, song_name == "Borders")
borders_top_tokens = topfeatures(borders_tfidf)
# Answer: "we" with 36
borders_top_tokens
```

```{r}
# Question 2
# Let's choose the top token from Borders (Question 1): "we"
we = convert(dfm_select(rap_dfm,"we"), to='data.frame')
plot(we$we,xlab="Song Index in DFM", ylab="Uses of \"we\"")

# Bonus: How to see which songs have more than 30 uses of we? 
we30 = subset(we, we > 30)
```

```{r}
# Question 3
top100 <- topfeatures(rap_dfm, n = 100, scheme = 'docfreq')

{ 
plot(top100, ylim=c(-5, max(top100)), type='l', col='blue', lwd=2.5, ylab="Value", xlab = 'Feature rank')
lines(max(top100)/top100, col='red', lty=2, lwd=2)
lines(log(max(top100)/(top100+1)), col=alpha('green',0.5), lty=1, lwd=3)
legend(80, 500, legend=c("DF", "N/DF", "log(N/(DF+1)"), col=c("blue", "red", 'green'), lty=c(1,2,1), cex=0.8)
}


```

```{r}
# we can alo zoom in

{ 
plot(top100, ylim=c(-5, 10), type='l', col='blue', lwd=2.5, ylab="Value", xlab = 'Feature rank')
lines(max(top100)/top100, col='red', lty=2, lwd=2)
lines(log(max(top100)/(top100+1)), col=alpha('green',0.5), lty=1, lwd=3)
legend(1, 7.5, legend=c("DF", "N/DF", "log(N/(DF+1)"), col=c("blue", "red", 'green'), lty=c(1,2,1), cex=0.8)
}


```



## Task 4: Calculating and comparing readability scores

For the two corpora used in this tutorial - US speeches and UK rap lyrics - we are now interested in comparing the them on a range of text metrics.

Answer the questions below:

1. Which of the two corpora has the higher lexical diversity score?
2. For each corpus, which text has the lowest readability as measured through the "Automated Readability Index"? (Hint: have a look at the `readability` package and the wrapper function [here](https://github.com/ben-aaron188/r_helper_functions/blob/master/get_single_readability.R))
3. How did the readability of US presidents' speeches change over time?

```{r}
# Helper function
corpus_lex_div = function(corpus_dfm) {
  mean(textstat_lexdiv(corpus_dfm)$TTR)
}


# Question 1:
# Compare lexical diversity of US speeches and UK Rap music. 
# The values indicate that rap has more lexical diversity.
us_speech_ld = corpus_lex_div(us_dfm)
rap_ld = corpus_lex_div(rap_dfm)

```

```{r}
# Question 2:
# Compare readability of US speeches and UK Rap music
# Lowest readbility rap song: Wiley - Rubicon
# Lowest readability speech 1789 - Washington
us_readability = textstat_readability(US_speeches, measure="ARI")
rap_readability = textstat_readability(uk_rap_corpus, measure="ARI")

max_us = which.max(us_readability$ARI)
max_rap = which.max(rap_readability$ARI)

us_readability[max_us, 'document']
rap_readability[max_rap, 'document']
# note that this gives you the row in which the max. value is and the directly only selects the 'document' column (which in this case contains the ID we need to determine which document it is)
```


```{r}
# Question 3: 
# add Year colum for US data
# Readability has improved over time
us_readability$year = US_speeches$documents$Year
plot(us_readability$year, us_readability$ARI, pch=19)
```


Finally: we now want to understand the relationship between readability, text meta indicators and lexical diversity.

For each corpus separately: Calculate the Coleman-Liau index, the lexical diversity and the number of characters per word. Now plot each of the three combinations of these measures in a scatterplot. Did the findings confirm your expectation?

```{r}
# Coleman Liau Index
us_coleman = textstat_readability(US_speeches, measure="Coleman.Liau.short")$Coleman.Liau.short
rap_coleman = textstat_readability(uk_rap_corpus, measure="Coleman.Liau.short")$Coleman.Liau.short

# Lexical Diversity
us_ld_all = textstat_lexdiv(us_dfm)$TTR
rap_ld_all = textstat_lexdiv(rap_dfm)$TTR

# Characters per word
# us_cpw defined above
rap_cpw = cpw(uk_rap_corpus$documents$texts)

# US Plots
plot(us_coleman, us_ld_all, xlab = "Readability", ylab = "Lexical Diversity") # no correlation
plot(us_cpw, us_coleman, xlab = "Character per word", ylab = "Readability") # as characters perword increase, readability score increases (i.e. gets less readable)
plot(us_cpw, us_ld_all, xlab = "Characters per word", ylab = "Lexical Diversity") # no correlation

# Rap Plots
plot(rap_coleman, rap_ld_all, xlab = "Readability", ylab = "Lexical Diversity")
plot(rap_cpw, rap_coleman, xlab = "Character per word", ylab = "Readability") 
plot(rap_cpw, rap_ld_all, xlab = "Characters per word", ylab = "Lexical Diversity")
```


What does this tell you about the relationship between these three aspects of text?

---

## Homework

### Part 1: Text pre-processing

Among the various researchers-degrees-of-freedom in natural language processing are decisions about:

- the stemming of words
- the removal of punctuation
- the removal of stopwords
- lower-casing the corpus
- the sparsity trimming

Use a corpus of your choice and build a TFIDF matrix for the original corpus and all possible combinations from the decisions to-be-made above. All of these can be done using base R and quanteda. 

_Hint: for the sparsity trimming, have a look at [this function](https://quanteda.io/reference/dfm_trim.html)_.


### Part 2: Practising with the `quanteda` package

The R package {`quanteda`}(https://quanteda.io/index.html) is a powerful toolbox to deal with text data in R. You will likely need this package for the natural language processing part of your final project. Do learn more about this package, read through and replicate for yourself these two tutorials:

- [Examining literature with quanteda in R](https://quanteda.io/articles/pkgdown/replication/digital-humanities.html)
- [Analysing social media (Twitter) data with quanteda](https://quanteda.io/articles/pkgdown/examples/twitter.html)

### Part 3: Preparation for next week

Please ensure that you have installed and can load the following R packages:

- syuzhet
- spacyr

---
